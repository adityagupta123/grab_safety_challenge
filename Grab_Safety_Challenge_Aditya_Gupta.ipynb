{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grab_Safety_Challenge_Aditya_Gupta.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "deepl",
      "language": "python",
      "name": "deepl"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbGD5Vxutd7d",
        "colab_type": "text"
      },
      "source": [
        "**Enter the Path Name of the folder containing the feature files in the file_path **\n",
        "\n",
        "**Download the model (\"finalized_model.sav\") and save it the same folder **\n",
        "\n",
        "** Training AUC is 0.7012 using hyperparameter Optimization **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmAwY_ROtc78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = \"../data/features\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m__Fi20dfi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import glob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkQnzm97dflB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score,  roc_curve, auc, confusion_matrix\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "import lightgbm as lgb\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyJq3D1d9kpX",
        "colab_type": "text"
      },
      "source": [
        "Reading the ***Label*** file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQyZtkWqdflu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_label = pd.read_csv(\"/content/drive/My Drive/data/data/labels/part-00000-e9445087-aa0a-433b-a7f6-7f4c19d78ad6-c000.csv\", dtype = {'bookingID': 'uint64', 'label' : 'category'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaBytzO19yIx",
        "colab_type": "text"
      },
      "source": [
        "Reading the ***Features*** file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-V4BOhUdfmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_files = glob.glob(file_path + \"/*.csv\")\n",
        "\n",
        "li = []\n",
        "\n",
        "for filename in all_files:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0, dtype = {'bookingID':'uint64', 'Accuracy':'float32', 'Bearing':'float32', \n",
        "                                                                  'acceleration_x':'float32', 'acceleration_y':'float32',\n",
        "                                                                  'acceleration_z':'float32', 'gyro_x':'float32', \n",
        "                                                                  'gyro_y':'float32', 'gyro_z':'float32', 'second':'float32', \n",
        "                                                                  'Speed':'float32'})\n",
        "    li.append(df)\n",
        "\n",
        "df_sort = pd.concat(li, axis=0, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmNIaCKw9-aq",
        "colab_type": "text"
      },
      "source": [
        "Removing Duplicates from the Booking id.  \n",
        "If same Booking ID is classified as Rash and Non-Rash,  I have taken it as Rash Driven."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu1j_u2Zdfma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_label = df_label.sort_values(['bookingID'])\n",
        "# df_label = df_label.sort_values(['label'])\n",
        "# df_label = df_label.drop_duplicates(subset='bookingID', keep='last')\n",
        "# df_label = df_label.set_index('bookingID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdLsfyYa-ay4",
        "colab_type": "text"
      },
      "source": [
        "Data Massaging for the Features Table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FNBXnHsdfmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sort = df_sort.groupby('bookingID',as_index=False).apply(pd.DataFrame.sort_values, 'second')\n",
        "df_sort = df_sort.reset_index()\n",
        "df_sort = df_sort.drop(['level_0','level_1'], axis = 1)\n",
        "df_sort = df_sort.set_index('bookingID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgcefz4p-hDh",
        "colab_type": "text"
      },
      "source": [
        "Combining the Feature and the Label tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1cCR8Lodfmr",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kzvkacj-nTc",
        "colab_type": "text"
      },
      "source": [
        "The idea of Rash driving can be classified as combination of Speed and Movement, where Speed is defined as speed along the direction of the car and Movement is defined as angular change in the GPS Bearing. \n",
        "\n",
        "High speed alone may not be enough for Rash Driving and similarly angular change also will not define rash driving.  A combination of both Speed and Movement is needed to classify Rash driving.\n",
        "\n",
        "1. Movement : Change in the GPS Bearing across time will depict the the angular change per second in the direction of car.\n",
        "\n",
        "2. Sign: If change in movement is along the different direction (clockwise and then anticlockwise or anticlockwise and then clockwise) in two consecutive seconds, it is non-zero else it is zero.\n",
        "\n",
        "3.  Abs_Movement: Absolute Movement per second irrespective of direction(clockwise ot anticlockwise)\n",
        "\n",
        "4.   Total Movement: Sum of Movement per second over the duration of the booking \n",
        "\n",
        "5. Duration: Analysis showed that Higher proportion of bookings with higher duration are classified as Rash.\n",
        "\n",
        "5. Acceleration: Change in Speed per second\n",
        "\n",
        "6. gyro_xyz: Total Gyro Movement as square root of the sum of squares of gyro movment in x,y and z direction. This tells that the ride is bumpy\n",
        "\n",
        "7. Rolling Movement: Rolling Average of Movement\n",
        "\n",
        "8. Rash_Movement: If the Rolling Average of Movement is above Threshold cutoff for permissible movement, it is classified as Rash Movement else zero.\n",
        "\n",
        "9. Total_Movement_by_duration: Average Movement for the booking. It is calculates as Total Movement over Total Duration of the Booking.\n",
        "\n",
        "11. Rolling_Speed : Rolling Average of Movement\n",
        "\n",
        "12. Average Speed: Average Speed of the Booking\n",
        "\n",
        "13. Median Speed: Median Speed of the Booking\n",
        "\n",
        "14. Rash_ Speed:  If the Rolling Speed is above a certain threshold it is classified as 1 or else 0\n",
        "\n",
        "15. acc_movement: If the Acceleration is above a certain threshold it is classified as 1 or else 0\n",
        "\n",
        "16.  gyro_movement:  If the gyro_xyz is above a certain threshold it is classified as 1 or else 0\n",
        "\n",
        "17. sign_s: Instance where both Rash Speed and Sudden Change in direction are there\n",
        "\n",
        "18. a_m: Instance where both acc_movement and gyro_movement are there\n",
        "\n",
        "19. a_s: Instance where both acc_movement and Rash Speed  are there\n",
        "\n",
        "20. sign_m: Instance where both Rash Speed and Sudden Change in direction are there\n",
        "\n",
        "21. a_m_s:  Instance where both Rash Speed , Acc_movement and gyro_movement are there\n",
        "\n",
        "22. sign_a_m_s:  Instance where both Rash Speed , Acc_movement and gyro_movement are there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6xyEotVLkU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class PreProcessing(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"Custom Pre-Processing estimator for our use-case\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def feature_creation(self, df):\n",
        "    ## Movement Features\n",
        "    df['Movement'] = df.groupby(['bookingID'])['Bearing'].diff().fillna(0)\n",
        "    df['sign'] = df.Movement.map(np.sign).diff(periods=1).fillna(0)\n",
        "    df['Abs_Movement'] = abs(df['Movement'])\n",
        "    df['Delta_Movement'] = df.groupby(['bookingID'])['Abs_Movement'].cumsum()\n",
        "    df['Total_Movement'] = df.groupby('bookingID')['Abs_Movement'].transform('sum')\n",
        "\n",
        "    ## Time Features\n",
        "    df['duration'] = df.groupby('bookingID')['bookingID'].transform('count')\n",
        "\n",
        "\n",
        "    ## Speed Features\n",
        "    df['Acceleration'] = abs(df.groupby(['bookingID'])['Speed'].diff().fillna(0))\n",
        "\n",
        "\n",
        "    ## Gyro Features\n",
        "    df['gyro_xyz'] = np.sqrt(df['gyro_x']**2 + df['gyro_y']**2 + df['gyro_z']**2)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "  def feature_eng(self, df, period_of_movement = 2, thresh_gyro = 0.65, thresh_acc = 5, thresh_delta = 15, thresh_roll_speed = 20):\n",
        "\n",
        "    ## Movement Features\n",
        "    df['Rolling_movement'] = df.groupby(['bookingID'])['Abs_Movement'].apply(lambda x: x.rolling(period_of_movement, 1).mean())\n",
        "    df['rash_movement'] = 0\n",
        "    df.loc[df.Delta_Movement >= thresh_delta, 'rash_movement'] = 1\n",
        "\n",
        "    ## Time Features\n",
        "    df['Total_Movement_by_duration'] = df['Total_Movement']/df['duration']\n",
        "\n",
        "    ## Speed Features\n",
        "    df['Rolling_Speed'] = df.groupby(['bookingID'])['Speed'].apply(lambda x: x.rolling(period_of_movement, 1).mean())\n",
        "    df['Avg_Speed'] = df.groupby(['bookingID'])['Rolling_Speed'].transform('mean')\n",
        "    df['Median_Speed'] = df.groupby(['bookingID'])['Rolling_Speed'].transform('median')\n",
        "    df['rash_speed'] = 0\n",
        "    df.loc[df.Rolling_Speed >= thresh_roll_speed, 'rash_speed'] = 1\n",
        "\n",
        "    df[['Rolling_Speed', 'Avg_Speed','Median_Speed',\n",
        "              'Rolling_movement','Total_Movement_by_duration']] = df[['Rolling_Speed', 'Avg_Speed','Median_Speed',\n",
        "                                                                            'Rolling_movement','Total_Movement_by_duration']].apply(lambda x: x.astype('float32'))\n",
        "\n",
        "    df['acc_movement'] = 0\n",
        "    df.loc[df.Acceleration >= thresh_acc, 'acc_movement'] = 1\n",
        "\n",
        "    df['gyro_movement'] = 0\n",
        "    df.loc[df_merge.gyro_xyz >= thresh_gyro, 'gyro_movement'] = 1\n",
        "\n",
        "    df['sign_s'] = df['sign'] * df['rash_speed']\n",
        "    df['a_m'] = df['acc_movement'] * df['gyro_movement']\n",
        "    df['a_s'] = df['acc_movement'] * df['rash_speed']\n",
        "    df['sign_m'] = df['sign'] * df['gyro_movement']\n",
        "    df['a_s_m'] = df['acc_movement'] * df['rash_speed'] * df['gyro_movement']\n",
        "\n",
        "    df['sign_a_s_m'] = df['sign'] * df['a_s_m']\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "  def data_prep(self, df):\n",
        "\n",
        "    p = df.drop(['Accuracy', 'Bearing','acceleration_x', 'acceleration_y',\n",
        "                 'acceleration_z', 'gyro_x', 'gyro_y', 'gyro_z', 'second',\n",
        "                 'Speed'], axis = 1)\n",
        "\n",
        "    q = p.groupby('bookingID')['Movement', 'Delta_Movement',\n",
        "         'Rolling_movement', 'duration', 'Total_Movement',\n",
        "         'Total_Movement_by_duration', 'Acceleration', 'Rolling_Speed',\n",
        "         'Avg_Speed', 'Median_Speed'].mean().reset_index()\n",
        "\n",
        "    q = pd.merge(left = q,\n",
        "             right = df.groupby('bookingID')['sign','rash_movement', 'rash_speed','gyro_movement','sign_s', 'a_m','a_s','sign_m','a_s_m', 'sign_a_s_m'].sum().reset_index(),\n",
        "             left_on = 'bookingID',\n",
        "             right_on = 'bookingID'\n",
        "            )\n",
        "\n",
        "#     q = pd.merge(left = q,\n",
        "#                  right = df_label.reset_index(),\n",
        "#                  left_on = 'bookingID',\n",
        "#                  right_on='bookingID'\n",
        "#                 )\n",
        "\n",
        "    q['rm_freq_per_ride'] = np.round(np.log(q.rash_movement/q.duration + 1)*100, decimals = 2)\n",
        "    q['rs_freq_per_ride'] = np.round(np.log(q.rash_speed/q.duration + 1)*100, decimals = 2)\n",
        "    q['gm_freq_per_ride'] = np.round(np.log(q.gyro_movement/q.duration + 1)*1000 , decimals = 2)\n",
        "    q['sign_freq_per_ride'] = np.round(np.log(q.sign/q.duration + 1)*100 , decimals = 2)\n",
        "\n",
        "\n",
        "    #   q.duration = np.log(q.duration)\n",
        "    #   q.gyro_movement = np.log(q.gyro_movement + 1)\n",
        "    #   q.rash_movement = np.log(q.rash_movement + 1)\n",
        "    #   q.rash_speed = np.log(q.rash_speed + 1)\n",
        "\n",
        "    \n",
        "    return q\n",
        "\n",
        "\n",
        "\n",
        "  def transform(self, df):\n",
        "      \"\"\"Regular transform() that is a help for training, validation & testing datasets\n",
        "         (NOTE: The operations performed here are the ones that we did prior to this cell)\n",
        "      \"\"\"\n",
        "      \n",
        "      df = self.feature_creation(df)\n",
        "      \n",
        "      q = self.feature_eng(df_merge,\n",
        "                period_of_movement = 2,\n",
        "                thresh_acc = 1.3, \n",
        "                thresh_delta = 5, \n",
        "                thresh_roll_speed = 8\n",
        "               )\n",
        "      q = self.data_prep(q)\n",
        "      \n",
        "#       scaler = MinMaxScaler()\n",
        "#       q = scaler.fit_transform(q)\n",
        "      \n",
        "      return q\n",
        "\n",
        "  def fit(self, df, y=None, **fit_params):\n",
        "      \"\"\"Fitting the Training dataset & calculating the required values from train\n",
        "         e.g: We will need the mean of X_train['Loan_Amount_Term'] that will be used in\n",
        "              transformation of X_test\n",
        "      \"\"\"\n",
        "\n",
        "#       self.term_mean_ = df['Loan_Amount_Term'].mean()\n",
        "#       self.amt_mean_ = df['LoanAmount'].mean()\n",
        "      return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z8x99MeKJ8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess = PreProcessing()\n",
        "X = preprocess.transform(df_sort)\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpKFWsnWsObD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = file_path + 'finalized_model.sav'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WehRVQwGKKbv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8690916c-9cee-408e-c938-264e94da070c"
      },
      "source": [
        "loaded_model = joblib.load(filename)\n",
        "result = loaded_model.predict(X)\n",
        "# print(result)\n",
        "\n",
        "\n",
        "false_positive_rate, true_positive_rate, thresholds = roc_curve(y, result)\n",
        "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
        "print(roc_auc)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5067901147419026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yvJqfKTBqkN5",
        "colab": {}
      },
      "source": [
        "# X = q.drop('label', axis = 1)\n",
        "# y = q.label.astype('int')\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 42, stratify = y )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# d_train = lgb.Dataset(X_train, label=y_train)\n",
        "\n",
        "# params = {}\n",
        "# params['learning_rate'] = 0.003\n",
        "# params['boosting_type'] = 'gbdt'\n",
        "# params['objective'] = 'binary'\n",
        "# params['metric'] = 'auc'\n",
        "# params['sub_feature'] = 0.5\n",
        "# params['num_leaves'] = 10\n",
        "# params['min_data'] = 50\n",
        "# params['max_depth'] = 10\n",
        "\n",
        "# clf = lgb.train(params, d_train, 100)\n",
        "\n",
        "# # print_auc(clf, X_test, y_test)\n",
        "\n",
        "# predictions=clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQd_2p4OmB9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74221db7-a577-4e31-b8fd-94e2df710555"
      },
      "source": [
        "#   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)\n",
        "#   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
        "#   print(roc_auc)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7012745925925925\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}